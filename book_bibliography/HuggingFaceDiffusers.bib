@InProceedings{dettmers2023case,
  author       = {Dettmers, Tim and Zettlemoyer, Luke},
  booktitle    = {International Conference on Machine Learning},
  title        = {The case for 4-bit precision: k-bit inference scaling laws},
  year         = {2023},
  organization = {PMLR},
  pages        = {7750--7774},
}

@Book{goodfellow2016a,
  author          = {Goodfellow, I. and Bengio, Y. and Courville, A.},
  publisher       = {MIT Press},
  title           = {Deep Learning},
  year            = {2016},
  address         = {Cambridge, MA},
  citation-number = {1.},
  date            = {2016},
  language        = {yo},
}

@Book{rao2019a,
  author          = {Rao, D. and McMahan, B.},
  publisher       = {O'Reilly Media},
  title           = {Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning},
  year            = {2019},
  address         = {Sebastopol, CA},
  citation-number = {2.},
  date            = {2019},
  language        = {en},
}

@Book{rothman2021a,
  author          = {Rothman, D.},
  publisher       = {Packt Publishing},
  title           = {Transformers for Natural Language Processing: Build and Train State-of-the-Art Models},
  year            = {2021},
  address         = {Birmingham, UK},
  citation-number = {3.},
  date            = {2021},
  language        = {en},
}

@Book{jurafsky2009a,
  author          = {Jurafsky, D. and Martin, J.H.},
  publisher       = {Prentice Hall},
  title           = {Speech and Language Processing},
  year            = {2009},
  address         = {Upper Saddle River, NJ},
  edition         = {2nd},
  citation-number = {4.},
  date            = {2009},
  language        = {en},
}

@book{burkov2019a,
  citation-number = {5.},
  author = {Burkov, A.},
  title = {The Hundred-Page Machine Learning Book},
  publisher = {Andriy Burkov},
  date = {2019},
  language = {en},
  address = {Quebec City, Canada}
}

@book{goyal2018a,
  citation-number = {6.},
  author = {Goyal, P. and Pandey, S. and Jain, K.},
  title = {Deep Learning for Natural Language Processing: Creating Neural Networks with Python},
  publisher = {Packt Publishing},
  date = {2018},
  language = {en},
  address = {Birmingham, UK}
}

@book{g2019a,
  citation-number = {7.},
  author = {Géron, A.},
  title = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
  edition = {2nd},
  publisher = {O'Reilly Media},
  date = {2019},
  language = {en},
  address = {Sebastopol, CA}
}

@book{trask2019a,
  citation-number = {8.},
  author = {Trask, A.W.},
  title = {Grokking Deep Learning},
  publisher = {Manning Publications},
  date = {2019},
  language = {en},
  address = {Shelter Island, NY}
}

@Book{ng2018a,
  author          = {Ng, A.},
  title           = {Machine Learning Yearning},
  note            = {Online]. Available:},
  citation-number = {9.},
  date            = {2018},
  language        = {en},
  url             = {https://www.deeplearning.ai/machine-learning-yearning/},
}

@book{grus2019a,
  citation-number = {10.},
  author = {Grus, J.},
  title = {Data Science from Scratch: First Principles with Python},
  edition = {2nd},
  publisher = {O'Reilly Media},
  date = {2019},
  language = {en},
  address = {Sebastopol, CA}
}

@inproceedings{vaswani2017a,
  citation-number = {1.},
  author = {Vaswani, A.},
  title = {Attention is All You Need},
  date = {2017-12},
  pages = {5998–6008},
  language = {en},
  booktitle = {Proc. 31st Conf. Neural Information Processing Systems (NIPS 2017},
  address = {Long Beach, CA, USA}
}

@Article{devlin2019a,
  author          = {Devlin, J. and Chang, M. and Lee, K. and Toutanova, K.},
  title           = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  note            = {arXiv preprint arXiv:1810.04805,},
  arxiv           = {1810.04805},
  citation-number = {2.},
  date            = {2019},
  language        = {nl},
}

@Article{brown2020a,
  author          = {Brown, T.B.},
  title           = {Language Models are Few-Shot Learners},
  note            = {arXiv preprint arXiv:2005.14165,},
  arxiv           = {2005.14165},
  citation-number = {3.},
  date            = {2020},
  language        = {sq},
}

@Article{howard2018a,
  author          = {Howard, J. and Ruder, S.},
  title           = {Universal Language Model Fine-tuning for Text Classification},
  note            = {arXiv preprint arXiv:1801.06146,},
  arxiv           = {1801.06146},
  citation-number = {4.},
  date            = {2018},
  language        = {en},
}

@inproceedings{yang2019a,
  citation-number = {5.},
  author = {Yang, Z.},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  date = {2019},
  pages = {5754–5764},
  language = {en},
  booktitle = {Proc. 33rd Int. Conf. Machine Learning (ICML 2019},
  address = {Long Beach, CA, USA}
}

@Article{liu2019a,
  author          = {Liu, Y.},
  title           = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  note            = {arXiv preprint arXiv:1907.11692,},
  arxiv           = {1907.11692},
  citation-number = {6.},
  date            = {2019},
  language        = {hu},
}

@Article{sun2019a,
  author          = {Sun, Y.},
  title           = {ERNIE: Enhanced Representation through Knowledge Integration},
  note            = {arXiv preprint arXiv:1904.09223,},
  arxiv           = {1904.09223},
  citation-number = {7.},
  date            = {2019},
  language        = {pt},
}

@article{raffel2020a,
  citation-number = {8.},
  author = {Raffel, C.},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  volume = {21},
  pages = {1–67,},
  date = {2020},
  language = {en},
  journal = {Journal of Machine Learning Research},
  number = {140}
}

@Article{lewis2019a,
  author          = {Lewis, M.},
  title           = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  note            = {arXiv preprint arXiv:1910.13461,},
  arxiv           = {1910.13461},
  citation-number = {9.},
  date            = {2019},
  language        = {en},
}

@Article{lan2019a,
  author          = {Lan, Z.},
  title           = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  note            = {arXiv preprint arXiv:1909.11942,},
  arxiv           = {1909.11942},
  citation-number = {10.},
  date            = {2019},
  language        = {en},
}

@Misc{tensorflow2015-whitepaper,
  author       = {Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  howpublished = {document in a website},
  note         = {Software available from tensorflow.org},
  title        = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year         = {2015},
  doi          = {10.5281/zenodo.4724125},
  url          = {https://www.tensorflow.org/},
}

@misc{pytorch-a,
  citation-number = {4.},
  author = {PyTorch},
  title = {PyTorch},
  note = {Available:},
  url = {https://pytorch.org.},
  language = {en},
  type = {[Online].}
}

@misc{allennlp-a,
  citation-number = {5.},
  author = {AllenNLP},
  title = {AllenNLP},
  note = {Available:},
  url = {https://allennlp.org.},
  language = {en},
  type = {[Online].}
}

@misc{keras-a,
  citation-number = {6.},
  author = {Keras},
  title = {Keras},
  note = {Available:},
  url = {https://keras.io.},
  language = {en},
  type = {[Online].}
}

@misc{fastai-a,
  citation-number = {7.},
  author = {Fastai},
  title = {Fast.ai},
  note = {Available:},
  url = {https://www.fast.ai.},
  language = {en},
  type = {[Online].}
}

@misc{deeplearningai-a,
  citation-number = {8.},
  author = {DeepLearningAI},
  title = {DeepLearning.AI},
  note = {Available:},
  url = {https://www.deeplearning.ai.},
  language = {en},
  type = {[Online].}
}

@misc{openai-a,
  citation-number = {9.},
  author = {OpenAI},
  title = {OpenAI},
  note = {Available:},
  url = {https://www.openai.com.},
  language = {en},
  type = {[Online].}
}

@misc{google-a,
  citation-number = {10.},
  author = {Google, A.I.},
  title = {Google AI Blog},
  note = {Available:},
  url = {https://ai.googleblog.com.},
  language = {en},
  type = {[Online].}
}

@article{unknown-d,
  language = {en},
  journal = {Academic Journals and Conferences}
}

@Misc{unknown-e,
  note            = {Available:},
  title           = {Journal of Machine Learning Research (JMLR)},
  citation-number = {1.},
  language        = {en},
  type            = {[Online].},
  url             = {http://www.jmlr.org.},
}

@misc{unknown-f,
  citation-number = {2.},
  title = {Transactions of the Association for Computational Linguistics (TACL},
  note = {Available:},
  url = {https://transacl.org.},
  language = {en},
  type = {[Online].}
}

@Conference{unknown-g,
  booktitle       = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title           = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  citation-number = {3.},
  language        = {en},
  type            = {[Online].},
  url             = {https://2021.emnlp.org.},
}

@Conference{unknown-h,
  title           = {Association for Computational Linguistics (ACL) Conference},
  note            = {Available:},
  citation-number = {4.},
  language        = {en},
  type            = {[Online].},
  url             = {https://www.aclweb.org/aclwiki/ACL.},
}

@misc{unknown-i,
  citation-number = {5.},
  title = {Neural Information Processing Systems (NeurIPS},
  note = {Available:},
  url = {https://neurips.cc.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-j,
  citation-number = {6.},
  title = {International Conference on Learning Representations (ICLR},
  note = {Available:},
  url = {https://iclr.cc.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-k,
  citation-number = {7.},
  title = {IEEE Transactions on Neural Networks and Learning Systems},
  note = {Available:},
  url = {https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-l,
  citation-number = {8.},
  title = {Artificial Intelligence Journal},
  note = {Available:},
  url = {https://www.journals.elsevier.com/artificial-intelligence.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-m,
  citation-number = {9.},
  title = {Pattern Recognition Journal},
  note = {Available:},
  url = {https://www.journals.elsevier.com/pattern-recognition.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-n,
  citation-number = {10.},
  title = {Data Mining and Knowledge Discovery Journal},
  note = {Available:},
  url = {https://www.springer.com/journal/10618.},
  language = {en},
  type = {[Online].}
}

@misc{gradient-a,
  citation-number = {1.},
  author = {Gradient, The},
  title = {The Gradient Blog},
  note = {Available:},
  url = {https://thegradient.pub.},
  language = {en},
  type = {[Online].}
}

@misc{distill-a,
  citation-number = {2.},
  author = {Distill},
  title = {Distill},
  note = {Available:},
  url = {https://distill.pub.},
  language = {en},
  type = {[Online].}
}

@misc{science-a,
  citation-number = {3.},
  author = {Science, Towards Data},
  title = {Towards Data Science on Medium},
  note = {Available:},
  url = {https://towardsdatascience.com.},
  language = {en},
  type = {[Online].}
}

@misc{google-b,
  citation-number = {4.},
  author = {Google, A.I.},
  title = {The Official Google AI Blog},
  note = {Available:},
  url = {https://ai.googleblog.com.},
  language = {en},
  type = {[Online].}
}

@misc{openai-b,
  citation-number = {5.},
  author = {OpenAI},
  title = {OpenAI Blog},
  note = {Available:},
  url = {https://openai.com/blog.},
  language = {en},
  type = {[Online].}
}

@misc{a-a,
  citation-number = {6.},
  author = {A.W.S.},
  title = {AWS Machine Learning Blog},
  note = {Available:},
  url = {https://aws.amazon.com/blogs/machine-learning.},
  language = {en},
  type = {[Online].}
}

@misc{research-a,
  citation-number = {7.},
  author = {Research, Microsoft},
  title = {Microsoft Research Blog},
  note = {Available:},
  url = {https://www.microsoft.com/en-us/research/blog.},
  language = {en},
  type = {[Online].}
}

@misc{i-a,
  citation-number = {8.},
  author = {I.B.M.},
  title = {IBM Research Blog},
  note = {Available:},
  url = {https://www.ibm.com/blogs/research.},
  language = {en},
  type = {[Online].}
}

@misc{deepmind-a,
  citation-number = {9.},
  author = {DeepMind},
  title = {DeepMind Blog},
  note = {Available:},
  url = {https://deepmind.com/blog.},
  language = {en},
  type = {[Online].}
}

@Article{ng-a,
  author          = {Ng, A.},
  journal         = {Deep Learning AI},
  title           = {The Batch by Andrew Ng},
  citation-number = {10.},
  language        = {en},
  type            = {[Online].},
  url             = {https://www.deeplearning.ai/thebatch.},
}

@misc{unknown-q,
  citation-number = {1.},
  title = {Stanford Question Answering Dataset (SQuAD},
  note = {Available:},
  url = {https://rajpurkar.github.io/SQuAD-explorer.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-r,
  citation-number = {2.},
  title = {The General Language Understanding Evaluation (GLUE) Benchmark},
  note = {Available:},
  url = {https://gluebenchmark.com.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-s,
  citation-number = {3.},
  title = {CoNLL-2003 Named Entity Recognition},
  note = {Available:},
  url = {https://www.clips.uantwerpen.be/conll2003/ner.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-t,
  citation-number = {4.},
  title = {The Natural Language Decathlon (DecaNLP},
  note = {Available:},
  url = {https://decanlp.com.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-u,
  citation-number = {5.},
  title = {Kaggle Datasets},
  note = {Available:},
  url = {https://www.kaggle.com/datasets.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-v,
  citation-number = {6.},
  title = {UCI Machine Learning Repository},
  note = {Available:},
  url = {https://archive.ics.uci.edu/ml.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-w,
  citation-number = {7.},
  title = {Hugging Face Datasets Library},
  note = {Available:},
  url = {https://github.com/huggingface/datasets.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-x,
  citation-number = {8.},
  title = {TensorFlow Datasets},
  note = {Available:},
  url = {https://www.tensorflow.org/datasets.},
  language = {en},
  type = {[Online].}
}

@misc{unknown-y,
  citation-number = {9.},
  title = {Common Crawl},
  note = {Available:},
  url = {https://commoncrawl.org.},
  language = {en},
  type = {[Online].}
}

@misc{imagenet-a,
  citation-number = {10.},
  author = {Imagenet},
  note = {Online]. Available:},
  url = {http://image-net.org.},
  language = {en}
}

@misc{university-a,
  citation-number = {11.},
  author = {University, Stanford},
  title = {Natural Language Processing with Deep Learning (CS224n},
  note = {Available:},
  url = {https://online.stanford.edu/courses/cs224n-natural-language-processing-deep-learning.},
  language = {en},
  type = {[Online].}
}

@misc{coursera-a,
  citation-number = {12.},
  author = {Coursera},
  title = {Natural Language Processing Specialization},
  note = {Available:},
  url = {https://www.coursera.org/specializations/natural-language-processing.},
  language = {en},
  type = {[Online].}
}

@misc{fastai-b,
  citation-number = {13.},
  author = {Fastai},
  title = {Practical Deep Learning for Coders},
  note = {Available:},
  url = {https://course.fast.ai.},
  language = {en},
  type = {[Online].}
}

@misc{deeplearningai-b,
  citation-number = {14.},
  author = {DeepLearningAI},
  title = {TensorFlow Developer Professional Certificate},
  note = {Available:},
  url = {https://www.coursera.org/professional-certificates/tensorflow-in-practice.},
  language = {en},
  type = {[Online].}
}

@misc{udacity-a,
  citation-number = {15.},
  author = {Udacity},
  title = {Machine Learning Engineer Nanodegree},
  note = {Available:},
  url = {https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t.},
  language = {en},
  type = {[Online].}
}

@misc{edx-a,
  citation-number = {16.},
  author = {edX},
  title = {Microsoft Professional Program in Artificial Intelligence},
  note = {Available:},
  url = {https://www.edx.org/microsoft-professional-program-artificial-intelligence.},
  language = {en},
  type = {[Online].}
}

@misc{opencourseware-a,
  citation-number = {17.},
  author = {OpenCourseWare, M.I.T.},
  title = {Introduction to Deep Learning (6.S191},
  note = {Available:},
  url = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s191-introduction-to-deep-learning-january-iap-2020.},
  language = {en},
  type = {[Online].}
}

@misc{academy-a,
  citation-number = {18.},
  author = {Academy, Khan},
  title = {Introduction to Linear Algebra},
  note = {Available:},
  url = {https://www.khanacademy.org/math/linear-algebra.},
  language = {en},
  type = {[Online].}
}

@misc{unknowo-a,
  citation-number = {19.},
  publisher = {Harvard University},
  title = {CS50's Introduction to Artificial Intelligence with Python},
  note = {Available:},
  url = {https://cs50.harvard.edu/ai.},
  language = {en},
  type = {[Online].}
}

@misc{coursera-b,
  citation-number = {20.},
  author = {Coursera},
  title = {Reinforcement Learning Specialization},
  note = {Available:},
  url = {https://www.coursera.org/specializations/reinforcement-learning.},
  language = {en},
  type = {[Online].}
}

@Article{Dettmers2023,
  author        = {Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  title         = {SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
  year          = {2023},
  month         = jun,
  abstract      = {Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2306.03078},
  eprint        = {2306.03078},
  file          = {:Dettmers2023 - SpQR_ a Sparse Quantized Representation for near Lossless LLM Weight Compression.pdf:PDF:http\://arxiv.org/pdf/2306.03078v1},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Wolf2019a,
  author        = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  title         = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  year          = {2019},
  month         = oct,
  abstract      = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1910.03771},
  eprint        = {1910.03771},
  file          = {:Wolf2019a - HuggingFace's Transformers_ State of the Art Natural Language Processing.pdf:PDF:http\://arxiv.org/pdf/1910.03771v5},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Wolf2019,
  author        = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  title         = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  year          = {2019},
  month         = oct,
  abstract      = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1910.03771},
  eprint        = {1910.03771},
  file          = {:Wolf2019 - HuggingFace's Transformers_ State of the Art Natural Language Processing.pdf:PDF:http\://arxiv.org/pdf/1910.03771v5},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Dettmers2023a,
  author        = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  year          = {2023},
  month         = may,
  abstract      = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2305.14314},
  eprint        = {2305.14314},
  file          = {:Dettmers2023a - QLoRA_ Efficient Finetuning of Quantized LLMs.pdf:PDF:http\://arxiv.org/pdf/2305.14314v1},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Dettmers2022,
  author        = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  title         = {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  year          = {2022},
  month         = aug,
  abstract      = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2208.07339},
  eprint        = {2208.07339},
  file          = {:Dettmers2022 - LLM.int8()_ 8 Bit Matrix Multiplication for Transformers at Scale.pdf:PDF:http\://arxiv.org/pdf/2208.07339v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Dettmers2021,
  author        = {Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  title         = {8-bit Optimizers via Block-wise Quantization},
  year          = {2021},
  month         = oct,
  abstract      = {Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2110.02861},
  eprint        = {2110.02861},
  file          = {:Dettmers2021 - 8 Bit Optimizers Via Block Wise Quantization.pdf:PDF:http\://arxiv.org/pdf/2110.02861v2},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Comment{jabref-meta: databaseType:bibtex;}
